{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 경로 변경 및 작업환경 세팅"
      ],
      "metadata": {
        "id": "Qc3nlGvyf6jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./drive/MyDrive/pointnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wdJEY4ufxSO",
        "outputId": "5847cfa2-0c6f-4799-c75b-d8216ddd2b2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/pointnet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 컴퓨팅 환경 확인\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP5y5wOjuLpy",
        "outputId": "17c5ce96-08c0-4ea9-d792-29d6fb450c8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov  7 13:18:24 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    43W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 경로확인인\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk_qEVcUf5Xr",
        "outputId": "266ec92f-2036-4380-d893-cd6effcdb182"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cls  misc  ModelNet40  Sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요 패키지 다운로드드\n",
        "!pip install meshio\n",
        "!pip install plyfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZfYpYMmbLDA",
        "outputId": "3024789f-7983-423a-9aea-96b4f9103489"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting meshio\n",
            "  Downloading meshio-5.3.4-py3-none-any.whl (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from meshio) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from meshio) (4.13.0)\n",
            "Collecting rich\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->meshio) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->meshio) (3.10.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->meshio) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 9.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: commonmark, rich, meshio\n",
            "Successfully installed commonmark-0.9.1 meshio-5.3.4 rich-12.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plyfile\n",
            "  Downloading plyfile-0.7.4-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from plyfile) (1.21.6)\n",
            "Installing collected packages: plyfile\n",
            "Successfully installed plyfile-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "os.path.abspath('')\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import json\n",
        "import meshio\n",
        "from plyfile import PlyData, PlyElement"
      ],
      "metadata": {
        "id": "Ad6TlmoPkmZq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset code"
      ],
      "metadata": {
        "id": "XvLc2mCBbl40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_segmentation_classes(root):\n",
        "    catfile = os.path.join(root, 'synsetoffset2category.txt')\n",
        "    cat = {}\n",
        "    meta = {}\n",
        "\n",
        "    with open(catfile, 'r') as f:\n",
        "        for line in f:\n",
        "            ls = line.strip().split()\n",
        "            cat[ls[0]] = ls[1]\n",
        "\n",
        "    for item in cat:\n",
        "        dir_seg = os.path.join(root, cat[item], 'points_label')\n",
        "        dir_point = os.path.join(root, cat[item], 'points')\n",
        "        fns = sorted(os.listdir(dir_point))\n",
        "        meta[item] = []\n",
        "        for fn in fns:\n",
        "            token = (os.path.splitext(os.path.basename(fn))[0])\n",
        "            meta[item].append((os.path.join(dir_point, token + '.pts'), os.path.join(dir_seg, token + '.seg')))\n",
        "    \n",
        "    with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), './misc/num_seg_classes.txt'), 'w') as f:\n",
        "        for item in cat:\n",
        "            datapath = []\n",
        "            num_seg_classes = 0\n",
        "            for fn in meta[item]:\n",
        "                datapath.append((item, fn[0], fn[1]))\n",
        "\n",
        "            for i in tqdm(range(len(datapath))):\n",
        "                l = len(np.unique(np.loadtxt(datapath[i][-1]).astype(np.uint8)))\n",
        "                if l > num_seg_classes:\n",
        "                    num_seg_classes = l\n",
        "\n",
        "            print(\"category {} num segmentation classes {}\".format(item, num_seg_classes))\n",
        "            f.write(\"{}\\t{}\\n\".format(item, num_seg_classes))\n",
        "\n",
        "def gen_modelnet_id(root):\n",
        "    classes = []\n",
        "    with open(os.path.join(root, 'train.txt'), 'r') as f:\n",
        "        for line in f:\n",
        "            classes.append(line.strip().split('/')[0])\n",
        "    classes = np.unique(classes)\n",
        "    with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), './misc/modelnet_id.txt'), 'w') as f:\n",
        "        for i in range(len(classes)):\n",
        "            f.write('{}\\t{}\\n'.format(classes[i], i))\n",
        "\n",
        "class ShapeNetDataset(data.Dataset):\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 npoints=2500,\n",
        "                 classification=False,\n",
        "                 class_choice=None,\n",
        "                 split='train',\n",
        "                 data_augmentation=True):\n",
        "        self.npoints = npoints\n",
        "        self.root = root\n",
        "        self.catfile = os.path.join(self.root, 'synsetoffset2category.txt')\n",
        "        self.cat = {}\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.classification = classification\n",
        "        self.seg_classes = {}\n",
        "        \n",
        "        with open(self.catfile, 'r') as f:\n",
        "            for line in f:\n",
        "                ls = line.strip().split()\n",
        "                self.cat[ls[0]] = ls[1]\n",
        "        #print(self.cat)\n",
        "        if not class_choice is None:\n",
        "            self.cat = {k: v for k, v in self.cat.items() if k in class_choice}\n",
        "\n",
        "        self.id2cat = {v: k for k, v in self.cat.items()}\n",
        "\n",
        "        self.meta = {}\n",
        "        splitfile = os.path.join(self.root, 'train_test_split', 'shuffled_{}_file_list.json'.format(split))\n",
        "        #from IPython import embed; embed()\n",
        "        filelist = json.load(open(splitfile, 'r'))\n",
        "        for item in self.cat:\n",
        "            self.meta[item] = []\n",
        "\n",
        "        for file in filelist:\n",
        "            _, category, uuid = file.split('/')\n",
        "            if category in self.cat.values():\n",
        "                self.meta[self.id2cat[category]].append((os.path.join(self.root, category, 'points', uuid+'.pts'),\n",
        "                                        os.path.join(self.root, category, 'points_label', uuid+'.seg')))\n",
        "\n",
        "        self.datapath = []\n",
        "        for item in self.cat:\n",
        "            for fn in self.meta[item]:\n",
        "                self.datapath.append((item, fn[0], fn[1]))\n",
        "\n",
        "        self.classes = dict(zip(sorted(self.cat), range(len(self.cat))))\n",
        "        print(self.classes)\n",
        "        with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), './misc/num_seg_classes.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                ls = line.strip().split()\n",
        "                self.seg_classes[ls[0]] = int(ls[1])\n",
        "        self.num_seg_classes = self.seg_classes[list(self.cat.keys())[0]]\n",
        "        print(self.seg_classes, self.num_seg_classes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fn = self.datapath[index]\n",
        "        cls = self.classes[self.datapath[index][0]]\n",
        "        point_set = np.loadtxt(fn[1]).astype(np.float32)\n",
        "        seg = np.loadtxt(fn[2]).astype(np.int64)\n",
        "        #print(point_set.shape, seg.shape)\n",
        "\n",
        "        choice = np.random.choice(len(seg), self.npoints, replace=True)\n",
        "        #resample\n",
        "        point_set = point_set[choice, :]\n",
        "\n",
        "        point_set = point_set - np.expand_dims(np.mean(point_set, axis = 0), 0) # center\n",
        "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis = 1)),0)\n",
        "        point_set = point_set / dist #scale\n",
        "\n",
        "        if self.data_augmentation:\n",
        "            theta = np.random.uniform(0,np.pi*2)\n",
        "            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\n",
        "            point_set[:,[0,2]] = point_set[:,[0,2]].dot(rotation_matrix) # random rotation\n",
        "            point_set += np.random.normal(0, 0.02, size=point_set.shape) # random jitter\n",
        "\n",
        "        seg = seg[choice]\n",
        "        point_set = torch.from_numpy(point_set)\n",
        "        seg = torch.from_numpy(seg)\n",
        "        cls = torch.from_numpy(np.array([cls]).astype(np.int64))\n",
        "\n",
        "        if self.classification:\n",
        "            return point_set, cls\n",
        "        else:\n",
        "            return point_set, seg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datapath)\n",
        "\n",
        "class ModelNetDataset(data.Dataset):\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 npoints=2500,\n",
        "                 split='train',\n",
        "                 data_augmentation=True,\n",
        "                 convert_off_to_ply=True):\n",
        "        self.npoints = npoints\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.fns = []\n",
        "        with open(os.path.join(root, '{}.txt'.format(self.split)), 'r') as f:\n",
        "            for line in f:\n",
        "                self.fns.append(line.strip())\n",
        "\n",
        "        self.cat = {}\n",
        "        with open(os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), './misc/modelnet_id.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                ls = line.strip().split()\n",
        "                self.cat[ls[0]] = int(ls[1])\n",
        "\n",
        "        print(self.cat)\n",
        "        self.classes = list(self.cat.keys())\n",
        "\n",
        "        if convert_off_to_ply:\n",
        "            p = Path(self.root)\n",
        "            print('searching path', p, 'to convert .off files to .ply')\n",
        "            l = list(p.glob('**/*.off'))\n",
        "            for in_file in l:\n",
        "                # print(in_file)\n",
        "                with in_file.open() as f:\n",
        "                    # some OFF files in original dataset had OFF345 345 344 where \n",
        "                    # OFF collided with the number. Needs \\n\n",
        "                    lines = f.readlines()\n",
        "                if lines[0] != 'OFF\\n':\n",
        "                    # print(lines[0:3])\n",
        "                    lines[0] = lines[0][0:3] + '\\n' + lines[0][3:]\n",
        "                    # print(lines[0:3])\n",
        "                    with in_file.open('w') as f:\n",
        "                        lines = \"\".join(lines)\n",
        "                        f.write(lines)\n",
        "\n",
        "                mesh = meshio.read(in_file, file_format=\"off\")\n",
        "                # print(in_file.replace(in_file.with_suffix('.ply')))\n",
        "                # print(in_file.with_suffix('').with_suffix('.ply'))\n",
        "                out_file = in_file.with_suffix('').with_suffix('.ply')\n",
        "                # print(out_file)\n",
        "                mesh.write(out_file, file_format='ply')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fn = self.fns[index]\n",
        "        cls = self.cat[fn.split('/')[0]]\n",
        "        with open(os.path.join(self.root, fn), 'rb') as f:\n",
        "            plydata = PlyData.read(f)\n",
        "        pts = np.vstack([plydata['vertex']['x'], plydata['vertex']['y'], plydata['vertex']['z']]).T\n",
        "        choice = np.random.choice(len(pts), self.npoints, replace=True)\n",
        "        point_set = pts[choice, :]\n",
        "\n",
        "        point_set = point_set - np.expand_dims(np.mean(point_set, axis=0), 0)  # center\n",
        "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis=1)), 0)\n",
        "        point_set = point_set / dist  # scale\n",
        "\n",
        "        if self.data_augmentation:\n",
        "            theta = np.random.uniform(0, np.pi * 2)\n",
        "            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "            point_set[:, [0, 2]] = point_set[:, [0, 2]].dot(rotation_matrix)  # random rotation\n",
        "            point_set += np.random.normal(0, 0.02, size=point_set.shape)  # random jitter\n",
        "\n",
        "        point_set = torch.from_numpy(point_set.astype(np.float32))\n",
        "        cls = torch.from_numpy(np.array([cls]).astype(np.int64))\n",
        "        return point_set, cls\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fns)\n",
        "\n",
        "# 우리 데이터셋을 위한 dataloader\n",
        "class TreeDataset(data.Dataset):\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 npoints=2500,\n",
        "                 split='train',\n",
        "                 data_augmentation=True,\n",
        "                 convert_off_to_ply=True):\n",
        "        self.npoints = npoints\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.fns = []\n",
        "        with open(os.path.join(root, '{}.txt'.format(self.split)), 'r') as f:\n",
        "            for line in f:\n",
        "                self.fns.append(line.strip())\n",
        "\n",
        "        self.cat = {}\n",
        "        with open(os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), './misc/tree_id.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                ls = line.strip().split()\n",
        "                self.cat[ls[0]] = int(ls[1])\n",
        "\n",
        "        print(self.cat)\n",
        "        self.classes = list(self.cat.keys())\n",
        "\n",
        "        if convert_off_to_ply:\n",
        "            p = Path(self.root)\n",
        "            print('searching path', p, 'to convert .off files to .ply')\n",
        "            l = list(p.glob('**/*.off'))\n",
        "            for in_file in l:\n",
        "                # print(in_file)\n",
        "                with in_file.open() as f:\n",
        "                    # some OFF files in original dataset had OFF345 345 344 where \n",
        "                    # OFF collided with the number. Needs \\n\n",
        "                    lines = f.readlines()\n",
        "                if lines[0] != 'OFF\\n':\n",
        "                    # print(lines[0:3])\n",
        "                    lines[0] = lines[0][0:3] + '\\n' + lines[0][3:]\n",
        "                    # print(lines[0:3])\n",
        "                    with in_file.open('w') as f:\n",
        "                        lines = \"\".join(lines)\n",
        "                        f.write(lines)\n",
        "\n",
        "                mesh = meshio.read(in_file, file_format=\"off\")\n",
        "                # print(in_file.replace(in_file.with_suffix('.ply')))\n",
        "                # print(in_file.with_suffix('').with_suffix('.ply'))\n",
        "                out_file = in_file.with_suffix('').with_suffix('.ply')\n",
        "                # print(out_file)\n",
        "                mesh.write(out_file, file_format='ply')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fn = self.fns[index]\n",
        "        cls = self.cat[fn.split('/')[0]]\n",
        "        with open(os.path.join(self.root, fn), 'rb') as f:\n",
        "            plydata = PlyData.read(f)\n",
        "        pts = np.vstack([plydata['vertex']['x'], plydata['vertex']['y'], plydata['vertex']['z']]).T\n",
        "        choice = np.random.choice(len(pts), self.npoints, replace=True)\n",
        "        point_set = pts[choice, :]\n",
        "\n",
        "        point_set = point_set - np.expand_dims(np.mean(point_set, axis=0), 0)  # center\n",
        "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis=1)), 0)\n",
        "        point_set = point_set / dist  # scale\n",
        "\n",
        "        if self.data_augmentation:\n",
        "            theta = np.random.uniform(0, np.pi * 2)\n",
        "            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "            point_set[:, [0, 2]] = point_set[:, [0, 2]].dot(rotation_matrix)  # random rotation\n",
        "            point_set += np.random.normal(0, 0.02, size=point_set.shape)  # random jitter\n",
        "\n",
        "        point_set = torch.from_numpy(point_set.astype(np.float32))\n",
        "        cls = torch.from_numpy(np.array([cls]).astype(np.int64))\n",
        "        return point_set, cls\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fns)\n"
      ],
      "metadata": {
        "id": "Y9-iqLK7bolk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "0z5RdwRWheoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class STN3d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(STN3d, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 9)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.size()[0]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n",
        "        if x.is_cuda:\n",
        "            iden = iden.cuda()\n",
        "        x = x + iden\n",
        "        x = x.view(-1, 3, 3)\n",
        "        return x\n",
        "\n",
        "\n",
        "class STNkd(nn.Module):\n",
        "    def __init__(self, k=64):\n",
        "        super(STNkd, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, k*k)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.size()[0]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
        "        if x.is_cuda:\n",
        "            iden = iden.cuda()\n",
        "        x = x + iden\n",
        "        x = x.view(-1, self.k, self.k)\n",
        "        return x\n",
        "\n",
        "class PointNetfeat(nn.Module):\n",
        "    def __init__(self, global_feat = True, feature_transform = False):\n",
        "        super(PointNetfeat, self).__init__()\n",
        "        self.stn = STN3d()\n",
        "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.global_feat = global_feat\n",
        "        self.feature_transform = feature_transform\n",
        "        if self.feature_transform:\n",
        "            self.fstn = STNkd(k=64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_pts = x.size()[2]\n",
        "        trans = self.stn(x)\n",
        "        x = x.transpose(2, 1)\n",
        "        x = torch.bmm(x, trans)\n",
        "        x = x.transpose(2, 1)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        if self.feature_transform:\n",
        "            trans_feat = self.fstn(x)\n",
        "            x = x.transpose(2,1)\n",
        "            x = torch.bmm(x, trans_feat)\n",
        "            x = x.transpose(2,1)\n",
        "        else:\n",
        "            trans_feat = None\n",
        "\n",
        "        pointfeat = x\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "        if self.global_feat:\n",
        "            return x, trans, trans_feat\n",
        "        else:\n",
        "            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
        "            return torch.cat([x, pointfeat], 1), trans, trans_feat\n",
        "\n",
        "class PointNetCls(nn.Module):\n",
        "    def __init__(self, k=2, feature_transform=False):\n",
        "        super(PointNetCls, self).__init__()\n",
        "        self.feature_transform = feature_transform\n",
        "        self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, k)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, trans, trans_feat = self.feat(x)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1), trans, trans_feat\n",
        "\n",
        "\n",
        "class PointNetDenseCls(nn.Module):\n",
        "    def __init__(self, k = 2, feature_transform=False):\n",
        "        super(PointNetDenseCls, self).__init__()\n",
        "        self.k = k\n",
        "        self.feature_transform=feature_transform\n",
        "        self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform)\n",
        "        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n",
        "        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.size()[0]\n",
        "        n_pts = x.size()[2]\n",
        "        x, trans, trans_feat = self.feat(x)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.conv4(x)\n",
        "        x = x.transpose(2,1).contiguous()\n",
        "        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n",
        "        x = x.view(batchsize, n_pts, self.k)\n",
        "        return x, trans, trans_feat\n",
        "\n",
        "def feature_transform_regularizer(trans):\n",
        "    d = trans.size()[1]\n",
        "    batchsize = trans.size()[0]\n",
        "    I = torch.eye(d)[None, :, :]\n",
        "    if trans.is_cuda:\n",
        "        I = I.cuda()\n",
        "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "A1aFX9S_hdha"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "V8ahbOVEbqcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper parameter\n",
        "# colab+: batchSize = 256\n",
        "# colab : bachtSize = 64\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '--batchSize', type=int, default=256, help='input batch size')\n",
        "parser.add_argument(\n",
        "    '--num_points', type=int, default=2500, help='input batch size')\n",
        "parser.add_argument(\n",
        "    '--workers', type=int, help='number of data loading workers', default=4)\n",
        "parser.add_argument(\n",
        "    '--nepoch', type=int, default=1, help='number of epochs to train for')\n",
        "parser.add_argument('--outf', type=str, default='cls', help='output folder')\n",
        "parser.add_argument('--model', type=str, default='', help='model path')\n",
        "parser.add_argument('--dataset', type=str, default = './Sample', help=\"dataset path\")\n",
        "parser.add_argument('--dataset_type', type=str, default='tree', help=\"dataset type shapenet|modelnet40\")\n",
        "parser.add_argument('--feature_transform', action='store_true', help=\"use feature transform\")\n",
        "\n",
        "opt = parser.parse_args(args = [])\n",
        "print(opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqrGQu1oZRXN",
        "outputId": "501038c3-bb3b-4532-8ae2-1a8cc22b39f6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batchSize=256, dataset='./Sample', dataset_type='tree', feature_transform=False, model='', nepoch=1, num_points=2500, outf='cls', workers=4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
        "\n",
        "opt.manualSeed = random.randint(1, 10000)  # fix seed\n",
        "print(\"Random Seed: \", opt.manualSeed)\n",
        "random.seed(opt.manualSeed)\n",
        "torch.manual_seed(opt.manualSeed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl6merZdZ4qt",
        "outputId": "d296d1b0-0f1e-4b6c-8d06-c79883007bb4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  900\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f880f9f8390>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset select code, 1분정도 소요됨\n",
        "if opt.dataset_type == 'shapenet':\n",
        "    dataset = ShapeNetDataset(\n",
        "        root=opt.dataset,\n",
        "        classification=True,\n",
        "        npoints=opt.num_points)\n",
        "\n",
        "    test_dataset = ShapeNetDataset(\n",
        "        root=opt.dataset,\n",
        "        classification=True,\n",
        "        split='test',\n",
        "        npoints=opt.num_points,\n",
        "        data_augmentation=False)\n",
        "elif opt.dataset_type == 'modelnet40':\n",
        "    dataset = ModelNetDataset(\n",
        "        root=opt.dataset,\n",
        "        npoints=opt.num_points,\n",
        "        split='train')\n",
        "\n",
        "    test_dataset = ModelNetDataset(\n",
        "        root=opt.dataset,\n",
        "        split='val',\n",
        "        npoints=opt.num_points,\n",
        "        data_augmentation=False)\n",
        "elif opt.dataset_type == 'tree':\n",
        "    dataset = TreeDataset(\n",
        "        root=opt.dataset,\n",
        "        npoints=opt.num_points,\n",
        "        split='trainval')\n",
        "\n",
        "    test_dataset = TreeDataset(\n",
        "        root=opt.dataset,\n",
        "        split='trainval',\n",
        "        npoints=opt.num_points,\n",
        "        data_augmentation=False)\n",
        "else:\n",
        "    exit('wrong dataset type')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJn7GAZbapAQ",
        "outputId": "d7d7bdf3-31d1-4fe6-92c0-28e954cb6ab3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'larch': 0, 'pine': 1}\n",
            "searching path Sample to convert .off files to .ply\n",
            "{'larch': 0, 'pine': 1}\n",
            "searching path Sample to convert .off files to .ply\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=opt.batchSize,\n",
        "    shuffle=True,\n",
        "    num_workers=int(opt.workers))\n",
        "\n",
        "testdataloader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=opt.batchSize,\n",
        "        shuffle=True,\n",
        "        num_workers=int(opt.workers))\n",
        "\n",
        "print(\"train:\", len(dataset), \"test\", len(test_dataset))\n",
        "num_classes = len(dataset.classes)\n",
        "print('classes', num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtYXO6XUgqy8",
        "outputId": "d43d16e8-e228-42bb-9a56-5da56e399af5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 33 test 33\n",
            "classes 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.makedirs(opt.outf)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "classifier = PointNetCls(k=num_classes, feature_transform=opt.feature_transform)\n",
        "\n",
        "if opt.model != '':\n",
        "    classifier.load_state_dict(torch.load(opt.model))\n",
        "\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "classifier.cuda()\n",
        "\n",
        "num_batch = len(dataset) / opt.batchSize"
      ],
      "metadata": {
        "id": "y5G6Y15shEHn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "B3WLarQ4hwjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(opt.nepoch):\n",
        "    scheduler.step()\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        points, target = data\n",
        "        target = target[:, 0]\n",
        "        points = points.transpose(2, 1)\n",
        "        points, target = points.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        classifier = classifier.train()\n",
        "        pred, trans, trans_feat = classifier(points)\n",
        "        loss = F.nll_loss(pred, target)\n",
        "        if opt.feature_transform:\n",
        "            loss += feature_transform_regularizer(trans_feat) * 0.001\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred_choice = pred.data.max(1)[1]\n",
        "        correct = pred_choice.eq(target.data).cpu().sum()\n",
        "        if i%20 == 0:\n",
        "          print('[%d: %d/%d] train loss: %f accuracy: %f' % (epoch, i, num_batch, loss.item(), correct.item() / float(opt.batchSize)))\n",
        "\n",
        "# Valid 코드 다시 짜기 뭔가 엉성함\n",
        "    valid_correct = 0\n",
        "    valid_testset = 0\n",
        "\n",
        "    for i,data in tqdm(enumerate(testdataloader, 0)):\n",
        "      points, target = data\n",
        "      target = target[:, 0]\n",
        "      points = points.transpose(2, 1)\n",
        "      points, target = points.cuda(), target.cuda()\n",
        "      classifier = classifier.eval()\n",
        "      pred, _, _ = classifier(points)\n",
        "      pred_choice = pred.data.max(1)[1]\n",
        "      correct = pred_choice.eq(target.data).cpu().sum()\n",
        "      valid_correct += correct.item()\n",
        "      valid_testset += points.size()[0]\n",
        "    print(\"Valid accuracy {}\".format(valid_correct / float(valid_testset)))\n",
        "\n",
        "    torch.save(classifier.state_dict(), '%s/cls_model_%d.pth' % (opt.outf, epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qqi2QtKhKPZ",
        "outputId": "1fbb62e2-9093-48d8-a011-1d4f4b887ea8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0: 0/0] train loss: 0.779789 accuracy: 0.062500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid accuracy 0.30303030303030304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Valid"
      ],
      "metadata": {
        "id": "ZKqGEkIVhyRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_correct = 0\n",
        "total_testset = 0\n",
        "for i,data in tqdm(enumerate(testdataloader, 0)):\n",
        "    points, target = data\n",
        "    target = target[:, 0]\n",
        "    points = points.transpose(2, 1)\n",
        "    points, target = points.cuda(), target.cuda()\n",
        "    classifier = classifier.eval()\n",
        "    pred, _, _ = classifier(points)\n",
        "    pred_choice = pred.data.max(1)[1]\n",
        "    correct = pred_choice.eq(target.data).cpu().sum()\n",
        "    total_correct += correct.item()\n",
        "    total_testset += points.size()[0]\n",
        "\n",
        "print(\"final accuracy {}\".format(total_correct / float(total_testset)))"
      ],
      "metadata": {
        "id": "QaZoQiWVhxkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415e9e34-6068-40c6-89d4-5b40845c4cf6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final accuracy 0.30303030303030304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7irCVOIcL0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}